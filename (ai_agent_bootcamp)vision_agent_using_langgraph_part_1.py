# -*- coding: utf-8 -*-
"""(AI Agent bootcamp)Vision Agent using Langgraph Part 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hP8woZzeD-UtYpXW9DouaT4n7hBKsf1M

# Agents in LangGraph

In this notebook, **we're going to build a simple agent using using LangGraph**.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q -U langchain_openai langchain_core langgraph

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q langfuse

import os
os.environ["OPENAI_API_KEY"] = ""
# Get keys for your project from the project settings page: https://cloud.langfuse.com
# Get keys for your project from the project settings page: https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = ""
os.environ["LANGFUSE_SECRET_KEY"] = ""
#os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com"  # ðŸ‡ªðŸ‡º EU region
os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region

from langfuse.langchain import CallbackHandler

# Initialize Langfuse CallbackHandler for LangGraph/Langchain (tracing)
langfuse_handler = CallbackHandler()

import base64
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

vision_llm = ChatOpenAI(model="gpt-4o")


def extract_text(img_path: str) -> str:
    """
    Extract text from an image file using a multimodal model.

    Args:
        img_path: A local image file path (strings).

    Returns:
        A single string containing the concatenated text extracted from each image.
    """
    all_text = ""
    try:

        # Read image and encode as base64
        with open(img_path, "rb") as image_file:
            image_bytes = image_file.read()

        image_base64 = base64.b64encode(image_bytes).decode("utf-8")

        # Prepare the prompt including the base64 image data
        message = [
            HumanMessage(
                content=[
                    {
                        "type": "text",
                        "text": (
                            "Extract all the text from this image. "
                            "Return only the extracted text, no explanations."
                        ),
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_base64}"
                        },
                    },
                ]
            )
        ]

        # Call the vision-capable model
        response = vision_llm.invoke(message)

        # Append extracted text
        all_text += response.content + "\n\n"

        return all_text.strip()
    except Exception as e:
        # You can choose whether to raise or just return an empty string / error message
        error_msg = f"Error extracting text: {str(e)}"
        print(error_msg)
        return ""


llm = ChatOpenAI(model="gpt-4o")


def divide(a: int, b: int) -> float:
    """Divide a and b."""
    return a / b

tools = [divide, extract_text]

llm_with_tools = llm.bind_tools(
    tools,
    parallel_tool_calls=False,
    #force_tool=True
)

"""Let's create our LLM and prompt it with the overall desired agent behavior."""

from typing import TypedDict, Annotated, Optional
from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages


class AgentState(TypedDict):
    # The input document
    input_file: Optional[str]  # Contains file path, type (PNG)
    messages: Annotated[list[AnyMessage], add_messages]

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.utils.function_calling import convert_to_openai_tool


def assistant(state: AgentState):
    # System message
    textual_description_of_tool = """
extract_text(img_path: str) -> str:
    Extract text from an image file using a multimodal model.

    Args:
        img_path: A local image file path (strings).

    Returns:
        A single string containing the concatenated text extracted from each image.
divide(a: int, b: int) -> float:
    Divide a and b
"""
    image = state["input_file"]
    sys_msg = SystemMessage(content=f"You are an helpful agent that can analyse some images and run some computations without provided tools :\n{textual_description_of_tool} \n You have access to some otpional images. Currently the loaded images is : {image}")

    return {"messages": [llm_with_tools.invoke([sys_msg] + state["messages"])], "input_file": state["input_file"]}

"""We define a `tools` node with our list of tools.

The `assistant` node is just our model with bound tools.

We create a graph with `assistant` and `tools` nodes.

We add `tools_condition` edge, which routes to `End` or to `tools` based on  whether the `assistant` calls a tool.

Now, we add one new step:

We connect the `tools` node *back* to the `assistant`, forming a loop.

* After the `assistant` node executes, `tools_condition` checks if the model's output is a tool call.
* If it is a tool call, the flow is directed to the `tools` node.
* The `tools` node connects back to `assistant`.
* This loop continues as long as the model decides to call tools.
* If the model response is not a tool call, the flow is directed to END, terminating the process.
"""

from langgraph.graph import START, StateGraph
from langgraph.prebuilt import ToolNode, tools_condition
from IPython.display import Image, display

# Graph
builder = StateGraph(AgentState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how the control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools
    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END
    tools_condition,
)
builder.add_edge("tools", "assistant")
react_graph = builder.compile()

# Show
display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))

messages = [HumanMessage(content="Divide 6790 by 5")]

messages = react_graph.invoke({"messages": messages, "input_file": None}, config={"callbacks": [langfuse_handler]})

for m in messages['messages']:
    m.pretty_print()

"""## Training program
MR Wayne left a note with his training program for the week. I came up with a recipe for dinner left in a note.

you can find the document [HERE](https://huggingface.co/datasets/agents-course/course-images/blob/main/en/unit2/LangGraph/Batman_training_and_meals.png), so download it and upload it in the local folder.

![Training](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/Batman_training_and_meals.png)
"""

# Create initial messages
messages = [HumanMessage(content="According to the note provided by MR Wayne in the provided images, what's the list of items I should buy for the dinner menu?")]

# Invoke react_graph with Langfuse callback integrated
messages = react_graph.invoke(
    {
        "messages": messages,
        "input_file": "Batman_training_and_meals.png"
    },
    config={"callbacks": [langfuse_handler]}
)

for m in messages['messages']:
    m.pretty_print()

# Create initial messages
messages = [HumanMessage(content="According to the note provided by MR Wayne in the provided images, what's the list of items I should buy for the dinner menu? Count the number of items and divide them by 50. Give resulting answer. Don't stop until you have integer answer")]

# Invoke react_graph with Langfuse callback integrated
messages = react_graph.invoke(
    {
        "messages": messages,
        "input_file": "Batman_training_and_meals.png"
    },
    config={"callbacks": [langfuse_handler]}
)

for m in messages['messages']:
    m.pretty_print()

# Create initial messages
messages = [HumanMessage(content="According to the note provided by MR Wayne in the provided images, what's the list of items I should buy for the dinner menu? Then send an email to the recipient email with the list of items")]

# Invoke react_graph with Langfuse callback integrated
messages = react_graph.invoke(
    {
        "messages": messages,
        "input_file": "Batman_training_and_meals.png"
    },
    config={"callbacks": [langfuse_handler]}
)